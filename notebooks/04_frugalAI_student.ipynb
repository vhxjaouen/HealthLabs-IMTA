{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347e8188",
   "metadata": {},
   "source": [
    "<img src=\"./IMTA.png\" alt=\"Logo IMT Atlantique\" width=\"300\"/>\n",
    "\n",
    "##  **Frugal AI : Data Scarcity on Prostate MRI**\n",
    "## TAF Health - UE B - 2025/2026 \n",
    "\n",
    "Pierre-Henri.Conze@imt-atlantique.fr - Vincent.Jaouen@imt-atlantique.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cff3c2",
   "metadata": {},
   "source": [
    "In this lab, we will work with the **Prostate158** dataset (Mid-Axial slices). \n",
    "We want to understand the impact of **training data size** and **augmentation** on segmentation performance.\n",
    "\n",
    "**Dataset**:\n",
    "*   Images: T2-weighted MRI slices of the prostate.\n",
    "*   Labels: Prostate segmentation masks.\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Setup**: Load `prostate158` dataset and define a fixed validation split.\n",
    "2.  **Part I (Scarcity)**: Train specific U-Nets on very small subsets (e.g., 5, 20 images) without augmentation.\n",
    "3.  **Part II (Augmentation)**: Repeat the training on the smallest subsets using extensive Data Augmentation.\n",
    "4.  **Part III (Semi-Supervised Learning)**: Use unlabeled data to improve performance.\n",
    "\n",
    "**Student Instructions:**\n",
    "*   This notebook contains several **Questions** and **Exercises** marked with üìù.\n",
    "*   Please complete the code cells marked with `TODO`.\n",
    "*   Answer the questions in the markdown cells provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfefb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "import os\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab. Setting up environment...\")\n",
    "    \n",
    "    repo_name = \"HealthLabs-IMTA\"\n",
    "    repo_url = \"https://github.com/vhxjaouen/HealthLabs-IMTA.git\"\n",
    "\n",
    "    # Go to root if we appear to be in the repo's notebooks directory\n",
    "    if os.path.basename(os.getcwd()) == \"notebooks\" and os.path.exists(\"../setup.py\"):\n",
    "        os.chdir(\"..\")\n",
    "        print(\"Moved to repository root.\")\n",
    "\n",
    "    # Check if we need to clone\n",
    "    if not os.path.exists(\"setup.py\"):\n",
    "        if not os.path.exists(repo_name):\n",
    "            print(f\"Cloning {repo_name}...\")\n",
    "            !git clone {repo_url}\n",
    "        \n",
    "        # Move into the repo\n",
    "        if os.path.exists(repo_name):\n",
    "            os.chdir(repo_name)\n",
    "            print(f\"Changed directory to {os.getcwd()}\")\n",
    "    \n",
    "    # Install package\n",
    "    if os.path.exists(\"setup.py\"):\n",
    "        print(\"Installing package...\")\n",
    "        !pip install .\n",
    "    else:\n",
    "        print(f\"Error: Could not find setup.py at {os.getcwd()}\")\n",
    "        print(f\"Directory contents: {os.listdir(os.getcwd())}\")\n",
    "\n",
    "    # Move to notebooks directory for running the rest of the notebook\n",
    "    if os.path.exists(\"notebooks\"):\n",
    "        os.chdir(\"notebooks\")\n",
    "        print(f\"Working directory set to: {os.getcwd()}\")\n",
    "    \n",
    "    print(\"Environment setup complete.\")\n",
    "else:\n",
    "    print(\"Running locally.\")\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityRangePercentilesd,\n",
    "    RandFlipd, RandRotate90d, RandZoomd, RandShiftIntensityd, RandGaussianNoised,\n",
    "    EnsureTyped, SpatialPadd, CenterSpatialCropd, Resized\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "set_determinism(seed=29200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ae665",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Inspection\n",
    "\n",
    "We will parse the `dataset.json` provided with the dataset to get image/label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_dir = \"../datasets/prostate158_MidAxial\"\n",
    "json_path = os.path.join(data_dir, \"dataset.json\")\n",
    "\n",
    "# Load dataset.json\n",
    "with open(json_path) as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "# Extract training paths (relative paths in JSON need to be joined with data_dir)\n",
    "data_dicts = []\n",
    "for entry in schema[\"training\"]:\n",
    "    # json entries: \"./imagesTr/xxx.nii.gz\"\n",
    "    img_path = os.path.join(data_dir, entry[\"image\"].replace(\"./\", \"\"))\n",
    "    lbl_path = os.path.join(data_dir, entry[\"label\"].replace(\"./\", \"\"))\n",
    "    data_dicts.append({\"image\": img_path, \"label\": lbl_path})\n",
    "\n",
    "print(f\"Total available images: {len(data_dicts)}\")\n",
    "\n",
    "# Define Fixed Split (Last 30 for Validation)\n",
    "val_size = 30\n",
    "val_files = data_dicts[-val_size:]\n",
    "train_pool = data_dicts[:-val_size]\n",
    "\n",
    "print(f\"Validation set size: {len(val_files)}\")\n",
    "print(f\"Training pool size: {len(train_pool)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04a020",
   "metadata": {},
   "source": [
    "### üìù Exercise 1: Visualize a Sample Pair\n",
    "\n",
    "It is always good practice to check your data before training.\n",
    "Complete the code below to load and visualize the **first image and its corresponding label** from the `train_pool`.\n",
    "*   Use `nibabel` (imported as `nib`) or just use the file path with `LoadImaged` transform if you prefer.\n",
    "*   Display them side-by-side using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6540030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "# Select the first sample\n",
    "sample = train_pool[0]\n",
    "print(f\"Sample: {sample}\")\n",
    "\n",
    "# TODO: Load the image and label using nib.load() or any other method\n",
    "# Note: nib.load(path).get_fdata() returns the numpy array\n",
    "# img_data = ...\n",
    "# label_data = ...\n",
    "\n",
    "# TODO: Visualize them using plt.imshow()\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(..., cmap=\"gray\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(..., cmap=\"jet\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb809f1",
   "metadata": {},
   "source": [
    "### üìù Question 1 (Data)\n",
    "What are the dimensions (shape) of the loaded image array? Is it 2D or 3D?\n",
    "Why does the `dataset.json` usually point to NIfTI (.nii.gz) files for medical imaging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc87600",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3a2ce",
   "metadata": {},
   "source": [
    "## 2. Transforms Pipeline (MONAI)\n",
    "\n",
    "We setup the MONAI transforms.\n",
    "*   **Preprocessing**: Load, Channel First, Normalize Intensity (1st-99th percentile).\n",
    "*   **Augmentation**: Flips, Rotation, Zoom, Intensity Shift (activated only if `augment=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42928af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(augment=False):\n",
    "    # Base Transforms\n",
    "    transforms_list = [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=-1), \n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(256, 256), mode=(\"bilinear\", \"nearest\")),\n",
    "        ScaleIntensityRangePercentilesd(\n",
    "            keys=\"image\", lower=1, upper=99, \n",
    "            b_min=0.0, b_max=1.0, clip=True\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    "    \n",
    "    # Augmentation\n",
    "    if augment:\n",
    "        transforms_list += [\n",
    "            # Geometric\n",
    "            RandFlipd(keys=[\"image\", \"label\"], spatial_axis=1, prob=0.5), \n",
    "            RandZoomd(keys=[\"image\", \"label\"], min_zoom=0.9, max_zoom=1.1, mode=[\"area\", \"nearest\"], prob=0.3),\n",
    "            \n",
    "            # Intensity\n",
    "            RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n",
    "            RandGaussianNoised(keys=[\"image\"], prob=0.1, mean=0.0, std=0.05),\n",
    "        ]\n",
    "        \n",
    "    return Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654fc38b",
   "metadata": {},
   "source": [
    "### üìù Question 2 (MONAI Transforms)\n",
    "Look at the `EnsureChannelFirstd` transform.\n",
    "PyTorch models expect tensors in the format `(Batch, Channel, Height, Width)` for 2D.\n",
    "Most standard 2D image libraries (like PIL or OpenCV) load images as `(H, W, C)` or `(H, W)`.\n",
    "Why is `EnsureChannelFirstd` important here, and what happens if an image is loaded as `(H, W)` without a channel dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137678d",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5685b",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Visualizing Augmentations\n",
    "\n",
    "Run the code below to see the effect of data augmentation on a single sample.\n",
    "Try running the cell multiple times to see different random transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with augmentation enabled\n",
    "# Note: CacheDataset allows us to cache the *deterministic* part of the transforms (first 4 items in list)\n",
    "# But here cache_rate=0.0 ensures we re-apply transforms every time to see randomness\n",
    "aug_ds = CacheDataset(data=[train_pool[0]], transform=get_transforms(augment=True), cache_rate=0.0)\n",
    "aug_loader = DataLoader(aug_ds, batch_size=1)\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(aug_loader))\n",
    "img_aug = batch[\"image\"][0, 0].numpy()\n",
    "lbl_aug = batch[\"label\"][0, 0].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Augmented Image\")\n",
    "plt.imshow(img_aug, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Augmented Label\")\n",
    "plt.imshow(lbl_aug, cmap=\"jet\", alpha=0.5) # Label overlay\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729332d9",
   "metadata": {},
   "source": [
    "## 3. Experiment Runner\n",
    "\n",
    "We reuse the configuration from `segmentation.yaml` but override channel settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from healthlabs_imta.utils.training import train_segmentation\n",
    "from healthlabs_imta.utils.model_utils import model_factory\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Load Config from package\n",
    "import healthlabs_imta\n",
    "package_dir = os.path.dirname(healthlabs_imta.__file__)\n",
    "config_path = os.path.join(package_dir, \"configs\", \"segmentation.yaml\")\n",
    "\n",
    "with open(config_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Override config for this dataset\n",
    "cfg[\"data\"][\"data_dir\"] = data_dir\n",
    "cfg[\"model\"][\"in_channels\"] = 1   # Single channel MRI\n",
    "cfg[\"model\"][\"out_channels\"] = 1  # Binary segmentation\n",
    "cfg[\"training\"][\"max_epochs\"] = 30  # Set global default duration\n",
    "\n",
    "def run_experiment(n_train_samples, augment, max_epochs=None):\n",
    "    # Use config value if max_epochs not provided\n",
    "    if max_epochs is None:\n",
    "        max_epochs = cfg[\"training\"][\"max_epochs\"]\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Running Experiment: N={n_train_samples}, Augmentation={augment}, Epochs={max_epochs}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # 1. Deterministic Subset\n",
    "    train_subset = train_pool[:n_train_samples]\n",
    "    \n",
    "    # 2. Dataloaders - Using CacheDataset to speed up training\n",
    "    train_ds = CacheDataset(\n",
    "        data=train_subset, \n",
    "        transform=get_transforms(augment=augment), \n",
    "        cache_rate=1.0, num_workers=2\n",
    "    )\n",
    "    val_ds = CacheDataset(\n",
    "        data=val_files, \n",
    "        transform=get_transforms(augment=False), \n",
    "        cache_rate=1.0, num_workers=2\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # 3. Model Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_factory(cfg[\"model\"]).to(device)\n",
    "    loss_fn = DiceCELoss(sigmoid=True, to_onehot_y=False)\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # 4. Training\n",
    "    history = train_segmentation(\n",
    "        model, train_loader, val_loader,\n",
    "        loss_fn, dice_metric, optimizer,\n",
    "        device=device, max_epochs=max_epochs,\n",
    "        overlay_fn=None # Disable plotting during loop\n",
    "    )\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3680534",
   "metadata": {},
   "source": [
    "### üìù Question 3 (MONAI Datasets)\n",
    "In `run_experiment`, we use `CacheDataset`. \n",
    "What is the advantage of using `CacheDataset` over a standard `Dataset` when `cache_rate=1.0`?\n",
    "When might you *not* want to use `CacheDataset` (or use a lower cache rate)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac3f32",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae2dbb",
   "metadata": {},
   "source": [
    "## 4. Part I: Scarcity Impact (No Augmentation)\n",
    "\n",
    "We will verify the hypothesis that **more data = better performance**.\n",
    "We will train the model with very few images: **5 and 20**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_no_aug = {}\n",
    "# Reduced sample sizes for the lab to save time\n",
    "sample_sizes = [20, 5] \n",
    "\n",
    "for n in sample_sizes:\n",
    "    # 30 epochs is enough for a quick demonstration\n",
    "    (train_losses, val_dices, best_dice, weights), _ = run_experiment(n, augment=False, max_epochs=30)\n",
    "    results_no_aug[n] = val_dices\n",
    "    print(f\"-> Final Best Dice (N={n}, No Aug): {best_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a117622",
   "metadata": {},
   "source": [
    "### üìù Question 4 (Results Analysis)\n",
    "Compare the performance (Best Dice) between N=5 and N=20. Is the difference significant? \n",
    "What is the risk of training a deep network like U-Net on only 5 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8831a13",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61239d63",
   "metadata": {},
   "source": [
    "## 5. Part II: Impact of Augmentation\n",
    "\n",
    "Now repeat the experiment for **N=5 and N=20** but with `augment=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_aug = {}\n",
    "aug_sample_sizes = [20, 5]\n",
    "\n",
    "# TODO: Complete the loop to run the experiment with augmentation enabled\n",
    "for n in aug_sample_sizes:\n",
    "    # Use run_experiment function\n",
    "    # (train_losses, val_dices, best_dice, weights), _ = ...\n",
    "    pass \n",
    "\n",
    "    # results_aug[n] = val_dices\n",
    "    # print(f\"-> Final Best Dice (N={n}, Aug): {best_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c98d5",
   "metadata": {},
   "source": [
    "### üìù Exercise 3: Plotting Results.\n",
    "\n",
    "Use the provided code to plot the learning curves.\n",
    "Does augmentation help more when N is small (5) or large (20)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "colors = {5: 'r', 20: 'blue'}\n",
    "\n",
    "# Plot No Aug\n",
    "for n, dices in results_no_aug.items():\n",
    "    c = colors.get(n, 'gray')\n",
    "    plt.plot(dices, label=f'N={n} (No Aug)', color=c, linestyle='-', linewidth=2)\n",
    "\n",
    "# Plot Aug\n",
    "# TODO: Uncomment and adapt if you filled results_aug\n",
    "# for n, dices in results_aug.items():\n",
    "#     c = colors.get(n, 'gray')\n",
    "#     plt.plot(dices, label=f'N={n} (Aug)', color=c, linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title(\"Prostate Segmentation: Impact of Scarcity & Augmentation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Dice\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6738d0a",
   "metadata": {},
   "source": [
    "### üìù Question 5 (Data Augmentation)\n",
    "In theory, data augmentation acts as a regularizer.\n",
    "Did you observe reduced overfitting (e.g., gap between training loss and validation metric) or improved generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d932599",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05440680",
   "metadata": {},
   "source": [
    "## 7. Part III: Semi-Supervised Learning (Student-Teacher)\n",
    "\n",
    "We simulate a scenario where we have **100 images** available, but only **50 are labeled**.\n",
    "Can we leverage the 50 \"unlabeled\" images to improve the performance of a model trained on only 50 labeled examples?\n",
    "\n",
    "**Strategy (Self-Training / Pseudo-Labeling):**\n",
    "1.  **Train Teacher**: Use the labeled images to train a model (Teacher).\n",
    "2.  **Generate Pseudo-Labels**: Use the Teacher to predict segmentation masks for the unlabeled images.\n",
    "3.  **Train Student**: Train a new model (Student) on the **combined dataset** (Labeled + Pseudo-Labeled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Resize\n",
    "import nibabel as nib\n",
    "import shutil\n",
    "\n",
    "n_labeled = 20       # Reduced to 20 for this lab exercise\n",
    "n_unlabeled = 30     # Use 30 unlabeled images\n",
    "ssl_epochs = 30      \n",
    "\n",
    "print(f\"Configuration: {n_labeled} Labeled, {n_unlabeled} Unlabeled, {ssl_epochs} Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454aff7",
   "metadata": {},
   "source": [
    "### Step 1: Train the Teacher\n",
    "We train a teacher on the small labeled set (N=20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68801a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Step 1: Training Teacher (N={n_labeled}) ---\")\n",
    "(teacher_losses, teacher_dices, teacher_best_dice, teacher_weights), teacher_model = run_experiment(n_labeled, augment=True, max_epochs=ssl_epochs)\n",
    "print(f\"Teacher Best Validation Dice: {teacher_best_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3aee77",
   "metadata": {},
   "source": [
    "### Step 2: Generate Pseudo-Labels\n",
    "We use the teacher to label the unlabeled images.\n",
    "Read the code below to understand how we resize the prediction back to the original image spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Step 2: Generating Pseudo-Labels for {n_unlabeled} images ---\")\n",
    "\n",
    "# Prepare Teacher for Inference\n",
    "teacher_model.load_state_dict(teacher_weights)\n",
    "teacher_model.eval()\n",
    "\n",
    "# Select Unlabeled Data (indices just after the labeled set)\n",
    "unlabeled_data = train_pool[n_labeled : n_labeled + n_unlabeled]\n",
    "\n",
    "# Setup Output Directory\n",
    "pseudo_label_dir = os.path.join(data_dir, \"pseudo_labels\")\n",
    "if os.path.exists(pseudo_label_dir):\n",
    "    shutil.rmtree(pseudo_label_dir)\n",
    "os.makedirs(pseudo_label_dir, exist_ok=True)\n",
    "\n",
    "pseudo_labeled_data = []\n",
    "infer_transforms = get_transforms(augment=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, item in enumerate(unlabeled_data):\n",
    "        # 1. Prediction (256x256)\n",
    "        temp_item = {\"image\": item[\"image\"], \"label\": item[\"image\"]} \n",
    "        input_data = infer_transforms(temp_item)\n",
    "        input_tensor = input_data[\"image\"].unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        outputs = teacher_model(input_tensor)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        start_mask = (outputs > 0.5).float().cpu().numpy()[0, 0] # Binary mask\n",
    "        \n",
    "        # 2. Resizing back to original resolution\n",
    "        # To ensure we don't cheat, we load the original NIfTI to get its shape/affine\n",
    "        orig_img = nib.load(item[\"image\"])\n",
    "        # Simplified manual resize for clarity:\n",
    "        resizer = Resize(spatial_size=orig_img.shape[:2], mode=\"nearest\")\n",
    "        mask_tensor_out = resizer(torch.from_numpy(start_mask).unsqueeze(0))\n",
    "        final_mask = mask_tensor_out.squeeze(0).numpy()\n",
    "        \n",
    "        # Correct dimensions (Add channel dim if needed)\n",
    "        if len(orig_img.shape) == 3 and orig_img.shape[2] == 1:\n",
    "            final_mask = final_mask[:, :, np.newaxis]\n",
    "            \n",
    "        # Save output\n",
    "        pseudo_filename = f\"pseudo_{os.path.basename(item['image'])}\"\n",
    "        pseudo_path = os.path.join(pseudo_label_dir, pseudo_filename)\n",
    "        nib.save(nib.Nifti1Image(final_mask.astype(np.float32), orig_img.affine), pseudo_path)\n",
    "        pseudo_labeled_data.append({\"image\": item[\"image\"], \"label\": pseudo_path})\n",
    "\n",
    "print(f\"Generated {len(pseudo_labeled_data)} pseudo-labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3e892",
   "metadata": {},
   "source": [
    "### üìù Exercise 4: Visualize a Pseudo-Label\n",
    "Visualize one of the generated pseudo-labels overlaid on its image. \n",
    "Does the teacher model make mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f872584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the first element of 'pseudo_labeled_data'\n",
    "# Load image and pseudo-label using nib.load()\n",
    "# Display them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09658a",
   "metadata": {},
   "source": [
    "### Step 3: Train the Student\n",
    "Now we train the student on **N=20 (GT) + N=30 (Pseudo)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32648b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Step 3: Training Student (Total N={len(pseudo_labeled_data) + n_labeled}) ---\")\n",
    "\n",
    "# Combine Data\n",
    "combined_data = train_pool[:n_labeled] + pseudo_labeled_data\n",
    "\n",
    "# Student Training Set with Augmentation\n",
    "student_ds = CacheDataset(data=combined_data, transform=get_transforms(augment=True), cache_rate=1.0, num_workers=2)\n",
    "# TODO: Create the dataloaders\n",
    "student_loader = DataLoader(student_ds, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Validation components reusable\n",
    "val_ds = CacheDataset(data=val_files, transform=get_transforms(augment=False), cache_rate=1.0, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# Train Student\n",
    "student_model = model_factory(cfg[\"model\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-3)\n",
    "\n",
    "(student_losses, student_dices, student_best_dice, student_weights) = train_segmentation(\n",
    "    student_model, student_loader, val_loader,\n",
    "    loss_fn, dice_metric, optimizer,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\", max_epochs=ssl_epochs\n",
    ")\n",
    "\n",
    "print(f\"Student Best Validation Dice: {student_best_dice:.4f}\")\n",
    "print(f\"Improvement over Teacher: {student_best_dice - teacher_best_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c9369",
   "metadata": {},
   "source": [
    "### üìù Question 6 (SSL Analysis)\n",
    "Did the Student outperform the Teacher? If not, what could be the reasons?\n",
    "Consider the quality of the pseudo-labels you visualized in Exercise 4. \n",
    "What happens if the Teacher generates bad pseudo-labels?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai3.12",
   "language": "python",
   "name": "monai3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
